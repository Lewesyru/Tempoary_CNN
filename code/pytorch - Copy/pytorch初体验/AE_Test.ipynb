{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Installations\\Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:105: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/200], loss:0.1157, MSE_loss:0.0363\n",
      "epoch [2/200], loss:0.0862, MSE_loss:0.0261\n",
      "epoch [3/200], loss:0.0709, MSE_loss:0.0214\n",
      "epoch [4/200], loss:0.0645, MSE_loss:0.0192\n",
      "epoch [5/200], loss:0.0542, MSE_loss:0.0159\n",
      "epoch [6/200], loss:0.0538, MSE_loss:0.0159\n",
      "epoch [7/200], loss:0.0546, MSE_loss:0.0161\n",
      "epoch [8/200], loss:0.0542, MSE_loss:0.0160\n",
      "epoch [9/200], loss:0.0512, MSE_loss:0.0151\n",
      "epoch [10/200], loss:0.0499, MSE_loss:0.0145\n",
      "epoch [11/200], loss:0.0467, MSE_loss:0.0138\n",
      "epoch [12/200], loss:0.0431, MSE_loss:0.0125\n",
      "epoch [13/200], loss:0.0425, MSE_loss:0.0126\n",
      "epoch [14/200], loss:0.0424, MSE_loss:0.0124\n",
      "epoch [15/200], loss:0.0386, MSE_loss:0.0112\n",
      "epoch [16/200], loss:0.0431, MSE_loss:0.0126\n",
      "epoch [17/200], loss:0.0426, MSE_loss:0.0123\n",
      "epoch [18/200], loss:0.0381, MSE_loss:0.0112\n",
      "epoch [19/200], loss:0.0373, MSE_loss:0.0108\n",
      "epoch [20/200], loss:0.0402, MSE_loss:0.0118\n",
      "epoch [21/200], loss:0.0368, MSE_loss:0.0106\n",
      "epoch [22/200], loss:0.0386, MSE_loss:0.0113\n",
      "epoch [23/200], loss:0.0342, MSE_loss:0.0098\n",
      "epoch [24/200], loss:0.0413, MSE_loss:0.0120\n",
      "epoch [25/200], loss:0.0335, MSE_loss:0.0096\n",
      "epoch [26/200], loss:0.0350, MSE_loss:0.0100\n",
      "epoch [27/200], loss:0.0385, MSE_loss:0.0111\n",
      "epoch [28/200], loss:0.0358, MSE_loss:0.0102\n",
      "epoch [29/200], loss:0.0355, MSE_loss:0.0102\n",
      "epoch [30/200], loss:0.0340, MSE_loss:0.0099\n",
      "epoch [31/200], loss:0.0331, MSE_loss:0.0095\n",
      "epoch [32/200], loss:0.0334, MSE_loss:0.0096\n",
      "epoch [33/200], loss:0.0359, MSE_loss:0.0104\n",
      "epoch [34/200], loss:0.0361, MSE_loss:0.0102\n",
      "epoch [35/200], loss:0.0300, MSE_loss:0.0086\n",
      "epoch [36/200], loss:0.0311, MSE_loss:0.0089\n",
      "epoch [37/200], loss:0.0286, MSE_loss:0.0082\n",
      "epoch [38/200], loss:0.0307, MSE_loss:0.0088\n",
      "epoch [39/200], loss:0.0313, MSE_loss:0.0090\n",
      "epoch [40/200], loss:0.0302, MSE_loss:0.0085\n",
      "epoch [41/200], loss:0.0286, MSE_loss:0.0082\n",
      "epoch [42/200], loss:0.0298, MSE_loss:0.0084\n",
      "epoch [43/200], loss:0.0294, MSE_loss:0.0085\n",
      "epoch [44/200], loss:0.0316, MSE_loss:0.0092\n",
      "epoch [45/200], loss:0.0296, MSE_loss:0.0085\n",
      "epoch [46/200], loss:0.0305, MSE_loss:0.0088\n",
      "epoch [47/200], loss:0.0314, MSE_loss:0.0090\n",
      "epoch [48/200], loss:0.0283, MSE_loss:0.0081\n",
      "epoch [49/200], loss:0.0280, MSE_loss:0.0080\n",
      "epoch [50/200], loss:0.0287, MSE_loss:0.0082\n",
      "epoch [51/200], loss:0.0282, MSE_loss:0.0078\n",
      "epoch [52/200], loss:0.0321, MSE_loss:0.0092\n",
      "epoch [53/200], loss:0.0299, MSE_loss:0.0086\n",
      "epoch [54/200], loss:0.0299, MSE_loss:0.0086\n",
      "epoch [55/200], loss:0.0283, MSE_loss:0.0081\n",
      "epoch [56/200], loss:0.0280, MSE_loss:0.0080\n",
      "epoch [57/200], loss:0.0266, MSE_loss:0.0075\n",
      "epoch [58/200], loss:0.0282, MSE_loss:0.0080\n",
      "epoch [59/200], loss:0.0265, MSE_loss:0.0077\n",
      "epoch [60/200], loss:0.0278, MSE_loss:0.0079\n",
      "epoch [61/200], loss:0.0283, MSE_loss:0.0081\n",
      "epoch [62/200], loss:0.0259, MSE_loss:0.0073\n",
      "epoch [63/200], loss:0.0310, MSE_loss:0.0090\n",
      "epoch [64/200], loss:0.0276, MSE_loss:0.0078\n",
      "epoch [65/200], loss:0.0296, MSE_loss:0.0085\n",
      "epoch [66/200], loss:0.0299, MSE_loss:0.0084\n",
      "epoch [67/200], loss:0.0301, MSE_loss:0.0085\n",
      "epoch [68/200], loss:0.0288, MSE_loss:0.0083\n",
      "epoch [69/200], loss:0.0256, MSE_loss:0.0073\n",
      "epoch [70/200], loss:0.0270, MSE_loss:0.0078\n",
      "epoch [71/200], loss:0.0261, MSE_loss:0.0074\n",
      "epoch [72/200], loss:0.0270, MSE_loss:0.0077\n",
      "epoch [73/200], loss:0.0282, MSE_loss:0.0081\n",
      "epoch [74/200], loss:0.0278, MSE_loss:0.0080\n",
      "epoch [75/200], loss:0.0275, MSE_loss:0.0078\n",
      "epoch [76/200], loss:0.0268, MSE_loss:0.0077\n",
      "epoch [77/200], loss:0.0246, MSE_loss:0.0070\n",
      "epoch [78/200], loss:0.0259, MSE_loss:0.0074\n",
      "epoch [79/200], loss:0.0269, MSE_loss:0.0076\n",
      "epoch [80/200], loss:0.0270, MSE_loss:0.0077\n",
      "epoch [81/200], loss:0.0293, MSE_loss:0.0083\n",
      "epoch [82/200], loss:0.0287, MSE_loss:0.0081\n",
      "epoch [83/200], loss:0.0288, MSE_loss:0.0081\n",
      "epoch [84/200], loss:0.0275, MSE_loss:0.0078\n",
      "epoch [85/200], loss:0.0283, MSE_loss:0.0081\n",
      "epoch [86/200], loss:0.0278, MSE_loss:0.0080\n",
      "epoch [87/200], loss:0.0266, MSE_loss:0.0076\n",
      "epoch [88/200], loss:0.0272, MSE_loss:0.0076\n",
      "epoch [89/200], loss:0.0249, MSE_loss:0.0072\n",
      "epoch [90/200], loss:0.0273, MSE_loss:0.0077\n",
      "epoch [91/200], loss:0.0260, MSE_loss:0.0073\n",
      "epoch [92/200], loss:0.0270, MSE_loss:0.0076\n",
      "epoch [93/200], loss:0.0243, MSE_loss:0.0068\n",
      "epoch [94/200], loss:0.0252, MSE_loss:0.0071\n",
      "epoch [95/200], loss:0.0301, MSE_loss:0.0084\n",
      "epoch [96/200], loss:0.0271, MSE_loss:0.0077\n",
      "epoch [97/200], loss:0.0272, MSE_loss:0.0077\n",
      "epoch [98/200], loss:0.0262, MSE_loss:0.0073\n",
      "epoch [99/200], loss:0.0242, MSE_loss:0.0068\n",
      "epoch [100/200], loss:0.0240, MSE_loss:0.0069\n",
      "epoch [101/200], loss:0.0261, MSE_loss:0.0073\n",
      "epoch [102/200], loss:0.0242, MSE_loss:0.0069\n",
      "epoch [103/200], loss:0.0279, MSE_loss:0.0080\n",
      "epoch [104/200], loss:0.0230, MSE_loss:0.0065\n",
      "epoch [105/200], loss:0.0253, MSE_loss:0.0072\n",
      "epoch [106/200], loss:0.0249, MSE_loss:0.0070\n",
      "epoch [107/200], loss:0.0249, MSE_loss:0.0070\n",
      "epoch [108/200], loss:0.0273, MSE_loss:0.0077\n",
      "epoch [109/200], loss:0.0271, MSE_loss:0.0077\n",
      "epoch [110/200], loss:0.0260, MSE_loss:0.0072\n",
      "epoch [111/200], loss:0.0278, MSE_loss:0.0078\n",
      "epoch [112/200], loss:0.0255, MSE_loss:0.0072\n",
      "epoch [113/200], loss:0.0267, MSE_loss:0.0076\n",
      "epoch [114/200], loss:0.0272, MSE_loss:0.0078\n",
      "epoch [115/200], loss:0.0237, MSE_loss:0.0068\n",
      "epoch [116/200], loss:0.0236, MSE_loss:0.0067\n",
      "epoch [117/200], loss:0.0291, MSE_loss:0.0082\n",
      "epoch [118/200], loss:0.0264, MSE_loss:0.0075\n",
      "epoch [119/200], loss:0.0246, MSE_loss:0.0069\n",
      "epoch [120/200], loss:0.0263, MSE_loss:0.0074\n",
      "epoch [121/200], loss:0.0274, MSE_loss:0.0078\n",
      "epoch [122/200], loss:0.0236, MSE_loss:0.0067\n",
      "epoch [123/200], loss:0.0242, MSE_loss:0.0067\n",
      "epoch [124/200], loss:0.0232, MSE_loss:0.0065\n",
      "epoch [125/200], loss:0.0223, MSE_loss:0.0063\n",
      "epoch [126/200], loss:0.0248, MSE_loss:0.0071\n",
      "epoch [127/200], loss:0.0206, MSE_loss:0.0057\n",
      "epoch [128/200], loss:0.0249, MSE_loss:0.0070\n",
      "epoch [129/200], loss:0.0243, MSE_loss:0.0069\n",
      "epoch [130/200], loss:0.0265, MSE_loss:0.0075\n",
      "epoch [131/200], loss:0.0263, MSE_loss:0.0074\n",
      "epoch [132/200], loss:0.0280, MSE_loss:0.0078\n",
      "epoch [133/200], loss:0.0262, MSE_loss:0.0075\n",
      "epoch [134/200], loss:0.0266, MSE_loss:0.0075\n",
      "epoch [135/200], loss:0.0278, MSE_loss:0.0078\n",
      "epoch [136/200], loss:0.0277, MSE_loss:0.0079\n",
      "epoch [137/200], loss:0.0252, MSE_loss:0.0072\n",
      "epoch [138/200], loss:0.0265, MSE_loss:0.0076\n",
      "epoch [139/200], loss:0.0242, MSE_loss:0.0069\n",
      "epoch [140/200], loss:0.0251, MSE_loss:0.0071\n",
      "epoch [141/200], loss:0.0235, MSE_loss:0.0065\n",
      "epoch [142/200], loss:0.0251, MSE_loss:0.0069\n",
      "epoch [143/200], loss:0.0265, MSE_loss:0.0074\n",
      "epoch [144/200], loss:0.0249, MSE_loss:0.0069\n",
      "epoch [145/200], loss:0.0224, MSE_loss:0.0063\n",
      "epoch [146/200], loss:0.0246, MSE_loss:0.0070\n",
      "epoch [147/200], loss:0.0276, MSE_loss:0.0077\n",
      "epoch [148/200], loss:0.0245, MSE_loss:0.0068\n",
      "epoch [149/200], loss:0.0261, MSE_loss:0.0074\n",
      "epoch [150/200], loss:0.0248, MSE_loss:0.0071\n",
      "epoch [151/200], loss:0.0250, MSE_loss:0.0071\n",
      "epoch [152/200], loss:0.0221, MSE_loss:0.0063\n",
      "epoch [153/200], loss:0.0231, MSE_loss:0.0065\n",
      "epoch [154/200], loss:0.0253, MSE_loss:0.0073\n",
      "epoch [155/200], loss:0.0254, MSE_loss:0.0071\n",
      "epoch [156/200], loss:0.0238, MSE_loss:0.0067\n",
      "epoch [157/200], loss:0.0239, MSE_loss:0.0068\n",
      "epoch [158/200], loss:0.0247, MSE_loss:0.0069\n",
      "epoch [159/200], loss:0.0253, MSE_loss:0.0071\n",
      "epoch [160/200], loss:0.0248, MSE_loss:0.0070\n",
      "epoch [161/200], loss:0.0267, MSE_loss:0.0075\n",
      "epoch [162/200], loss:0.0246, MSE_loss:0.0069\n",
      "epoch [163/200], loss:0.0240, MSE_loss:0.0067\n",
      "epoch [164/200], loss:0.0259, MSE_loss:0.0074\n",
      "epoch [165/200], loss:0.0240, MSE_loss:0.0068\n",
      "epoch [166/200], loss:0.0233, MSE_loss:0.0066\n",
      "epoch [167/200], loss:0.0240, MSE_loss:0.0067\n",
      "epoch [168/200], loss:0.0223, MSE_loss:0.0064\n",
      "epoch [169/200], loss:0.0244, MSE_loss:0.0069\n",
      "epoch [170/200], loss:0.0274, MSE_loss:0.0076\n",
      "epoch [171/200], loss:0.0233, MSE_loss:0.0066\n",
      "epoch [172/200], loss:0.0249, MSE_loss:0.0071\n",
      "epoch [173/200], loss:0.0237, MSE_loss:0.0067\n",
      "epoch [174/200], loss:0.0265, MSE_loss:0.0075\n",
      "epoch [175/200], loss:0.0254, MSE_loss:0.0072\n",
      "epoch [176/200], loss:0.0248, MSE_loss:0.0069\n",
      "epoch [177/200], loss:0.0259, MSE_loss:0.0073\n",
      "epoch [178/200], loss:0.0260, MSE_loss:0.0072\n",
      "epoch [179/200], loss:0.0233, MSE_loss:0.0065\n",
      "epoch [180/200], loss:0.0249, MSE_loss:0.0071\n",
      "epoch [181/200], loss:0.0258, MSE_loss:0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [182/200], loss:0.0264, MSE_loss:0.0075\n",
      "epoch [183/200], loss:0.0252, MSE_loss:0.0071\n",
      "epoch [184/200], loss:0.0226, MSE_loss:0.0063\n",
      "epoch [185/200], loss:0.0237, MSE_loss:0.0068\n",
      "epoch [186/200], loss:0.0229, MSE_loss:0.0064\n",
      "epoch [187/200], loss:0.0223, MSE_loss:0.0065\n",
      "epoch [188/200], loss:0.0232, MSE_loss:0.0066\n",
      "epoch [189/200], loss:0.0255, MSE_loss:0.0072\n",
      "epoch [190/200], loss:0.0234, MSE_loss:0.0065\n",
      "epoch [191/200], loss:0.0231, MSE_loss:0.0066\n",
      "epoch [192/200], loss:0.0255, MSE_loss:0.0073\n",
      "epoch [193/200], loss:0.0266, MSE_loss:0.0076\n",
      "epoch [194/200], loss:0.0240, MSE_loss:0.0067\n",
      "epoch [195/200], loss:0.0233, MSE_loss:0.0066\n",
      "epoch [196/200], loss:0.0221, MSE_loss:0.0062\n",
      "epoch [197/200], loss:0.0229, MSE_loss:0.0064\n",
      "epoch [198/200], loss:0.0243, MSE_loss:0.0069\n",
      "epoch [199/200], loss:0.0218, MSE_loss:0.0061\n",
      "epoch [200/200], loss:0.0221, MSE_loss:0.0062\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "def plot_sample_img(img, name):\n",
    "    img = img.view(1, 28, 28)\n",
    "    save_image(img, './sample_{}.png'.format(name))\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "    transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 28 * 28),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "#         print(img)\n",
    "#         print(img.size())\n",
    "#         print('--------------------------')\n",
    "        img = img.view(img.size(0), -1)\n",
    "#         print(img)\n",
    "#         print(img.size())\n",
    "#         print('--------------------------')\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        #MIDIMG=model.encoder(img).view(img.size(0),8,8).cpu().detach().numpy()\n",
    "        #MIDIMG=MIDIMG[0]\n",
    "        #MIDIMG=np.rint(MIDIMG/np.sum(MIDIMG)*255)\n",
    "        #print(MIDIMG)\n",
    "        #plt.imshow(MIDIMG)\n",
    "        #plt.show()\n",
    "        loss = criterion(output, img)\n",
    "        MSE_loss = nn.MSELoss()(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "    if epoch % 20 == 0:\n",
    "        x = to_img(img.cpu().data)\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        save_image(x, './mlp_img/x_{}.png'.format(epoch))\n",
    "        save_image(x_hat, './mlp_img/x_hat_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoded IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Installations\\Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:106: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/500], loss:0.0573, MSE_loss:0.0170\n",
      "epoch [2/500], loss:0.0446, MSE_loss:0.0130\n",
      "epoch [3/500], loss:0.0340, MSE_loss:0.0099\n",
      "epoch [4/500], loss:0.0335, MSE_loss:0.0096\n",
      "epoch [5/500], loss:0.0259, MSE_loss:0.0074\n",
      "epoch [6/500], loss:0.0232, MSE_loss:0.0067\n",
      "epoch [7/500], loss:0.0212, MSE_loss:0.0060\n",
      "epoch [8/500], loss:0.0205, MSE_loss:0.0058\n",
      "epoch [9/500], loss:0.0230, MSE_loss:0.0066\n",
      "epoch [10/500], loss:0.0199, MSE_loss:0.0056\n",
      "epoch [11/500], loss:0.0189, MSE_loss:0.0053\n",
      "epoch [12/500], loss:0.0204, MSE_loss:0.0057\n",
      "epoch [13/500], loss:0.0192, MSE_loss:0.0055\n",
      "epoch [14/500], loss:0.0172, MSE_loss:0.0048\n",
      "epoch [15/500], loss:0.0188, MSE_loss:0.0052\n",
      "epoch [16/500], loss:0.0163, MSE_loss:0.0044\n",
      "epoch [17/500], loss:0.0154, MSE_loss:0.0042\n",
      "epoch [18/500], loss:0.0186, MSE_loss:0.0053\n",
      "epoch [19/500], loss:0.0146, MSE_loss:0.0040\n",
      "epoch [20/500], loss:0.0172, MSE_loss:0.0048\n",
      "epoch [21/500], loss:0.0158, MSE_loss:0.0044\n",
      "epoch [22/500], loss:0.0156, MSE_loss:0.0044\n",
      "epoch [23/500], loss:0.0157, MSE_loss:0.0043\n",
      "epoch [24/500], loss:0.0145, MSE_loss:0.0039\n",
      "epoch [25/500], loss:0.0137, MSE_loss:0.0037\n",
      "epoch [26/500], loss:0.0132, MSE_loss:0.0035\n",
      "epoch [27/500], loss:0.0120, MSE_loss:0.0033\n",
      "epoch [28/500], loss:0.0140, MSE_loss:0.0038\n",
      "epoch [29/500], loss:0.0141, MSE_loss:0.0038\n",
      "epoch [30/500], loss:0.0151, MSE_loss:0.0040\n",
      "epoch [31/500], loss:0.0133, MSE_loss:0.0035\n",
      "epoch [32/500], loss:0.0147, MSE_loss:0.0040\n",
      "epoch [33/500], loss:0.0124, MSE_loss:0.0033\n",
      "epoch [34/500], loss:0.0128, MSE_loss:0.0035\n",
      "epoch [35/500], loss:0.0119, MSE_loss:0.0032\n",
      "epoch [36/500], loss:0.0120, MSE_loss:0.0033\n",
      "epoch [37/500], loss:0.0124, MSE_loss:0.0033\n",
      "epoch [38/500], loss:0.0120, MSE_loss:0.0032\n",
      "epoch [39/500], loss:0.0126, MSE_loss:0.0034\n",
      "epoch [40/500], loss:0.0123, MSE_loss:0.0033\n",
      "epoch [41/500], loss:0.0121, MSE_loss:0.0032\n",
      "epoch [42/500], loss:0.0119, MSE_loss:0.0032\n",
      "epoch [43/500], loss:0.0141, MSE_loss:0.0038\n",
      "epoch [44/500], loss:0.0128, MSE_loss:0.0034\n",
      "epoch [45/500], loss:0.0112, MSE_loss:0.0029\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if not os.path.exists('./mlp_pansharpened'):\n",
    "    os.mkdir('./mlp_pansharpened')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "def plot_sample_img(img, name):\n",
    "    img = img.view(1, 28, 28)\n",
    "    save_image(img, './sample_{}.png'.format(name))\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "    transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 1764),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1764, 3136),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3136, 1764),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1764, 28 * 28),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "#         print(img)\n",
    "#         print(img.size())\n",
    "#         print('--------------------------')\n",
    "        img = img.view(img.size(0), -1)\n",
    "#         print(img)\n",
    "#         print(img.size())\n",
    "#         print('--------------------------')\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "#         MIDIMG=model.encoder(img).view(img.size(0),56,56).cpu().detach().numpy()\n",
    "#         MIDIMG=MIDIMG[0]\n",
    "#         MIDIMG=np.rint(MIDIMG/np.sum(MIDIMG)*255)\n",
    "#         print(MIDIMG)\n",
    "#         plt.imshow(MIDIMG)\n",
    "#         plt.show()\n",
    "        \n",
    "        loss = criterion(output, img)\n",
    "        MSE_loss = nn.MSELoss()(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "    if epoch % 1 == 0:\n",
    "        x = to_img(img.cpu().data)\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        MIDIMG= model.encoder(img).view(img.size(0),1,56,56).cpu().data\n",
    "        save_image(x, './mlp_pansharpened/x_{}.png'.format(epoch))\n",
    "        save_image(x_hat, './mlp_pansharpened/x_hat_{}.png'.format(epoch))\n",
    "        save_image(MIDIMG, './mlp_pansharpened/x_midlayer{}.png'.format(epoch))\n",
    "        save_image(img.view(img.size(0),1,28,28).cpu().data, './mlp_pansharpened/x_inputlayer{}.png'.format(epoch))\n",
    "torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Installations\\Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:110: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/50], loss:0.3814, MSE_loss:0.1137\n",
      "epoch [2/50], loss:0.3699, MSE_loss:0.1094\n",
      "epoch [3/50], loss:0.3611, MSE_loss:0.1059\n",
      "epoch [4/50], loss:0.3529, MSE_loss:0.1029\n",
      "epoch [5/50], loss:0.3532, MSE_loss:0.1030\n",
      "epoch [6/50], loss:0.3514, MSE_loss:0.1022\n",
      "epoch [7/50], loss:0.3465, MSE_loss:0.1006\n",
      "epoch [8/50], loss:0.3448, MSE_loss:0.0998\n",
      "epoch [9/50], loss:0.3469, MSE_loss:0.1005\n",
      "epoch [10/50], loss:0.3469, MSE_loss:0.1004\n",
      "epoch [11/50], loss:0.3477, MSE_loss:0.1007\n",
      "epoch [12/50], loss:0.3457, MSE_loss:0.1000\n",
      "epoch [13/50], loss:0.3411, MSE_loss:0.0981\n",
      "epoch [14/50], loss:0.3415, MSE_loss:0.0986\n",
      "epoch [15/50], loss:0.3452, MSE_loss:0.0996\n",
      "epoch [16/50], loss:0.3418, MSE_loss:0.0985\n",
      "epoch [17/50], loss:0.3401, MSE_loss:0.0980\n",
      "epoch [18/50], loss:0.3426, MSE_loss:0.0987\n",
      "epoch [19/50], loss:0.3444, MSE_loss:0.0993\n",
      "epoch [20/50], loss:0.3408, MSE_loss:0.0981\n",
      "epoch [21/50], loss:0.3413, MSE_loss:0.0984\n",
      "epoch [22/50], loss:0.3428, MSE_loss:0.0988\n",
      "epoch [23/50], loss:0.3375, MSE_loss:0.0971\n",
      "epoch [24/50], loss:0.3408, MSE_loss:0.0979\n",
      "epoch [25/50], loss:0.3331, MSE_loss:0.0953\n",
      "epoch [26/50], loss:0.3354, MSE_loss:0.0962\n",
      "epoch [27/50], loss:0.3397, MSE_loss:0.0976\n",
      "epoch [28/50], loss:0.3391, MSE_loss:0.0975\n",
      "epoch [29/50], loss:0.3351, MSE_loss:0.0959\n",
      "epoch [30/50], loss:0.3358, MSE_loss:0.0964\n",
      "epoch [31/50], loss:0.3431, MSE_loss:0.0988\n",
      "epoch [32/50], loss:0.3366, MSE_loss:0.0966\n",
      "epoch [33/50], loss:0.3393, MSE_loss:0.0974\n",
      "epoch [34/50], loss:0.3365, MSE_loss:0.0965\n",
      "epoch [35/50], loss:0.3405, MSE_loss:0.0980\n",
      "epoch [36/50], loss:0.3368, MSE_loss:0.0966\n",
      "epoch [37/50], loss:0.3406, MSE_loss:0.0980\n",
      "epoch [38/50], loss:0.3357, MSE_loss:0.0961\n",
      "epoch [39/50], loss:0.3393, MSE_loss:0.0975\n",
      "epoch [40/50], loss:0.3367, MSE_loss:0.0967\n",
      "epoch [41/50], loss:0.3346, MSE_loss:0.0960\n",
      "epoch [42/50], loss:0.3368, MSE_loss:0.0966\n",
      "epoch [43/50], loss:0.3378, MSE_loss:0.0968\n",
      "epoch [44/50], loss:0.3379, MSE_loss:0.0968\n",
      "epoch [45/50], loss:0.3367, MSE_loss:0.0966\n",
      "epoch [46/50], loss:0.3374, MSE_loss:0.0968\n",
      "epoch [47/50], loss:0.3331, MSE_loss:0.0952\n",
      "epoch [48/50], loss:0.3361, MSE_loss:0.0964\n",
      "epoch [49/50], loss:0.3375, MSE_loss:0.0969\n",
      "epoch [50/50], loss:0.3346, MSE_loss:0.0958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Installations\\Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:137: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_2 [1/50], loss:0.1047, MSE_loss:0.0323\n",
      "epoch_2 [2/50], loss:0.1022, MSE_loss:0.0316\n",
      "epoch_2 [3/50], loss:0.1008, MSE_loss:0.0313\n",
      "epoch_2 [4/50], loss:0.0999, MSE_loss:0.0310\n",
      "epoch_2 [5/50], loss:0.0993, MSE_loss:0.0309\n",
      "epoch_2 [6/50], loss:0.0988, MSE_loss:0.0308\n",
      "epoch_2 [7/50], loss:0.0985, MSE_loss:0.0307\n",
      "epoch_2 [8/50], loss:0.0982, MSE_loss:0.0306\n",
      "epoch_2 [9/50], loss:0.0980, MSE_loss:0.0305\n",
      "epoch_2 [10/50], loss:0.0978, MSE_loss:0.0305\n",
      "epoch_2 [11/50], loss:0.0976, MSE_loss:0.0304\n",
      "epoch_2 [12/50], loss:0.0974, MSE_loss:0.0304\n",
      "epoch_2 [13/50], loss:0.0972, MSE_loss:0.0303\n",
      "epoch_2 [14/50], loss:0.0971, MSE_loss:0.0303\n",
      "epoch_2 [15/50], loss:0.0970, MSE_loss:0.0302\n",
      "epoch_2 [16/50], loss:0.0970, MSE_loss:0.0302\n",
      "epoch_2 [17/50], loss:0.0968, MSE_loss:0.0302\n",
      "epoch_2 [18/50], loss:0.0967, MSE_loss:0.0301\n",
      "epoch_2 [19/50], loss:0.0966, MSE_loss:0.0301\n",
      "epoch_2 [20/50], loss:0.0965, MSE_loss:0.0300\n",
      "epoch_2 [21/50], loss:0.0965, MSE_loss:0.0300\n",
      "epoch_2 [22/50], loss:0.0964, MSE_loss:0.0300\n",
      "epoch_2 [23/50], loss:0.0963, MSE_loss:0.0300\n",
      "epoch_2 [24/50], loss:0.0961, MSE_loss:0.0299\n",
      "epoch_2 [25/50], loss:0.0961, MSE_loss:0.0299\n",
      "epoch_2 [26/50], loss:0.0960, MSE_loss:0.0298\n",
      "epoch_2 [27/50], loss:0.0959, MSE_loss:0.0298\n",
      "epoch_2 [28/50], loss:0.0958, MSE_loss:0.0298\n",
      "epoch_2 [29/50], loss:0.0957, MSE_loss:0.0298\n",
      "epoch_2 [30/50], loss:0.0957, MSE_loss:0.0297\n",
      "epoch_2 [31/50], loss:0.0956, MSE_loss:0.0297\n",
      "epoch_2 [32/50], loss:0.0955, MSE_loss:0.0297\n",
      "epoch_2 [33/50], loss:0.0954, MSE_loss:0.0297\n",
      "epoch_2 [34/50], loss:0.0953, MSE_loss:0.0297\n",
      "epoch_2 [35/50], loss:0.0953, MSE_loss:0.0296\n",
      "epoch_2 [36/50], loss:0.0952, MSE_loss:0.0296\n",
      "epoch_2 [37/50], loss:0.0951, MSE_loss:0.0296\n",
      "epoch_2 [38/50], loss:0.0950, MSE_loss:0.0295\n",
      "epoch_2 [39/50], loss:0.0949, MSE_loss:0.0295\n",
      "epoch_2 [40/50], loss:0.0949, MSE_loss:0.0295\n",
      "epoch_2 [41/50], loss:0.0948, MSE_loss:0.0295\n",
      "epoch_2 [42/50], loss:0.0947, MSE_loss:0.0295\n",
      "epoch_2 [43/50], loss:0.0947, MSE_loss:0.0295\n",
      "epoch_2 [44/50], loss:0.0946, MSE_loss:0.0294\n",
      "epoch_2 [45/50], loss:0.0946, MSE_loss:0.0294\n",
      "epoch_2 [46/50], loss:0.0946, MSE_loss:0.0294\n",
      "epoch_2 [47/50], loss:0.0945, MSE_loss:0.0294\n",
      "epoch_2 [48/50], loss:0.0945, MSE_loss:0.0294\n",
      "epoch_2 [49/50], loss:0.0945, MSE_loss:0.0294\n",
      "epoch_2 [50/50], loss:0.0944, MSE_loss:0.0294\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if not os.path.exists('./AE_denoise'):\n",
    "    os.mkdir('./AE_denoise')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "def plot_sample_img(img, name):\n",
    "    img = img.view(1, 28, 28)\n",
    "    save_image(img, './sample_{}.png'.format(name))\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "    transforms.Lambda(lambda tensor:tensor_round(tensor))\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', transform=img_transform, download=True)\n",
    "#test_dataset=MNIST('./data', transform=img_transform, train = False, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 196),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(196, 16),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 196),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(196, 28 * 28),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "#         print(img)\n",
    "#         print(img.size())\n",
    "#         print('--------------------------')\n",
    "        img = img.view(img.size(0), -1)\n",
    "        noise = (torch.randint(-10,2,(img.size(0),img.size(1)))>0).float()\n",
    "        noised_img=((img+noise)>0).float()\n",
    "#         print(img)\n",
    "#         print(img.size())\n",
    "#         print('--------------------------')\n",
    "        noised_img = Variable(noised_img).cuda()\n",
    "        \n",
    "        # ===================forward=====================\n",
    "        output = model(noised_img)\n",
    "#         MIDIMG=model.encoder(img).view(img.size(0),56,56).cpu().detach().numpy()\n",
    "#         MIDIMG=MIDIMG[0]\n",
    "#         MIDIMG=np.rint(MIDIMG/np.sum(MIDIMG)*255)\n",
    "#         print(MIDIMG)\n",
    "#         plt.imshow(MIDIMG)\n",
    "#         plt.show()\n",
    "        \n",
    "        loss = criterion(output, noised_img)\n",
    "        MSE_loss = nn.MSELoss()(output, noised_img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        #MIDIMG= model.encoder(img).view(img.size(0),1,56,56).cpu().data\n",
    "        save_image(x_hat, './AE_denoise/c_output_{}.png'.format(epoch+1))\n",
    "        #save_image(MIDIMG, './mlp_pansharpened/x_midlayer{}.png'.format(epoch))\n",
    "        save_image(img.view(img.size(0),1,28,28).cpu().data, './AE_denoise/a_raw{}.png'.format(epoch+1))\n",
    "        save_image(noised_img.view(img.size(0),1,28,28).cpu().data, './AE_denoise/b_noised{}.png'.format(epoch+1))\n",
    "        \n",
    "        \n",
    "        \n",
    "for epoch_2 in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        img, _ = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).cuda()\n",
    "\n",
    "        output = model(img)\n",
    "\n",
    "        loss = criterion(output, img)\n",
    "        MSE_loss = nn.MSELoss()(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch_2 [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch_2 + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "    if (epoch_2+1) % 10 == 0 or epoch_2 == 0:\n",
    "        x_hat = to_img(output.cpu().data)\n",
    "        save_image(x_hat, './AE_denoise/d_output_{}.png'.format(epoch_2+1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7266,  2.2476, -1.8607],\n",
      "        [-1.6655,  0.2462,  0.6537]])\n",
      "tensor([-1.7266,  2.2476, -1.8607, -1.6655,  0.2462,  0.6537])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.randn(2,3)\n",
    "print(x)\n",
    "print(x.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist=[i+1 for i in range(50)]\n",
    "alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
