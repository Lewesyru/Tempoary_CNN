{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "if not os.path.exists('./clouds_remove'):\n",
    "    os.mkdir('./clouds_remove')\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda tensor:min_max_normalization(tensor, 0, 1)),\n",
    "])\n",
    "        \n",
    "\n",
    "class LandsatDataset():\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "#         self.batch_size=batch_size\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         samples=[]\n",
    "#         for i in range(self.batch_size):\n",
    "        img_name = os.listdir(self.root_dir)[idx]#[idx*self.batch_size+i]\n",
    "        img = Image.open(self.root_dir+img_name)\n",
    "        image = numpy.array(img)[4000:4700,3000:3500]\n",
    "        image_tensor=torch.from_numpy(image)\n",
    "        image_tensor=image_tensor.cuda()\n",
    "        #samples.append(image_tensor)\n",
    "        #tensor=torch.cat((*samples),(0))\n",
    "#         if self.transform:\n",
    "#             sample = self.transform(sample)\n",
    "        return image_tensor\n",
    "\n",
    "num_epochs=10000\n",
    "learning_rate = 4*1e-4\n",
    "batch_size=55\n",
    "\n",
    "dataset = LandsatDataset('./landsat8_142_49_truecolor_copy/',transform=None)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(55, 55),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(55, 55),\n",
    "            nn.ReLU(True),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for data in dataloader:\n",
    "    \n",
    "    img_B = min_max_normalization(data[0:,0:,0:,0].float(),0.0,1.0)\n",
    "    img_G = min_max_normalization(data[0:,0:,0:,1].float(),0.0,1.0)\n",
    "    img_R = min_max_normalization(data[0:,0:,0:,2].float(),0.0,1.0)\n",
    "    img_B=img_B.view(img_B.size(0),-1).float()\n",
    "    img_B = torch.transpose(img_B, 0, 1)\n",
    "    img_G=img_G.view(img_G.size(0),-1).float()\n",
    "    img_G = torch.transpose(img_G, 0, 1)\n",
    "    img_R=img_R.view(img_R.size(0),-1).float()\n",
    "    img_R = torch.transpose(img_R, 0, 1)\n",
    "    for epoch in range(num_epochs):\n",
    "        img_B = Variable(img_B).cuda()\n",
    "        output_B = model(img_B)\n",
    "        output_G = model(img_G)\n",
    "        output_R = model(img_R)\n",
    "        loss = criterion(output_B, img_B)\n",
    "        MSE_loss = nn.MSELoss()(output_B, img_B)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "        if (epoch+1)%1000==0:\n",
    "            print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "              .format(epoch + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "            \n",
    "    \n",
    "#     img_fb= torch.transpose(img_B,0,1)\n",
    "#     img_fg= torch.transpose(img_G,0,1)\n",
    "#     img_fr= torch.transpose(img_R,0,1)\n",
    "    output_fb=torch.transpose(output_B,0,1).view(data.size(0),1,data.size(1),data.size(2))\n",
    "    output_fg=torch.transpose(output_G,0,1).view(data.size(0),1,data.size(1),data.size(2))\n",
    "    output_fr=torch.transpose(output_R,0,1).view(data.size(0),1,data.size(1),data.size(2))\n",
    "    output=torch.cat((output_fb,output_fg,output_fr),1).cpu().data\n",
    "    # MIDIMG= model.encoder(img).view(img.size(0),1,56,56).cpu().data\n",
    "    # save_image(x, './mlp_pansharpened/x_{}.png'.format(epoch))\n",
    "    save_image(output, './clouds_remove/x_output.png')   \n",
    "        \n",
    "    input_fb=torch.transpose(img_B,0,1).view(data.size(0),1,data.size(1),data.size(2))\n",
    "    input_fg=torch.transpose(img_G,0,1).view(data.size(0),1,data.size(1),data.size(2))\n",
    "    input_fr=torch.transpose(img_R,0,1).view(data.size(0),1,data.size(1),data.size(2))\n",
    "    ini=torch.cat((input_fb,input_fg,input_fr),1).cpu().data\n",
    "    save_image(ini, './clouds_remove/x_inputlayer.png')\n",
    "    # save_image(MIDIMG, './mlp_pansharpened/x_midlayer{}.png'.format(epoch))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-43db16977045>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[0min_band\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0min_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[0mband5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_band\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReadAsArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m \u001b[0mband5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_max_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mband5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-43db16977045>\u001b[0m in \u001b[0;36mmin_max_normalization\u001b[1;34m(tensor, min_value, max_value)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mmax_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_value\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(7, 3),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 7),\n",
    "            nn.ReLU(True),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.chdir('E:/pyScripts/pytorch/data/142_49/LC08_L1TP_142049_20170120_20170311_01_T1/')\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B1.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band1=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band1=min_max_normalization(band1,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B2.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band2=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band2=min_max_normalization(band2,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B3.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band3=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band3=min_max_normalization(band3,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B4.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band4=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band4=min_max_normalization(band4,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B5.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band5=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band5=min_max_normalization(band5,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B6.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band6=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band6=min_max_normalization(band6,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B7.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band7=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band7=min_max_normalization(band7,0.0,1.0)\n",
    "\n",
    "\n",
    "num_epochs=1\n",
    "learning_rate = 4*1e-4\n",
    "batch_size=55\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0,band1.size(0)):\n",
    "        b1_input=band1[i].view(-1,1)\n",
    "        b2_input=band2[i].view(-1,1)\n",
    "        b3_input=band3[i].view(-1,1)\n",
    "        b4_input=band4[i].view(-1,1)\n",
    "        b5_input=band5[i].view(-1,1)\n",
    "        b6_input=band6[i].view(-1,1)\n",
    "        b7_input=band7[i].view(-1,1)\n",
    "        bands_input = torch.cat((\n",
    "            b1_input,b2_input,b3_input,b4_input,b5_input,b6_input,b7_input),1)\n",
    "        bands_input=Variable(bands_input).cuda()\n",
    "        output=model(bands_input)\n",
    "        loss = criterion(output, bands_input)\n",
    "        MSE_loss = nn.MSELoss()(output, bands_input)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ===================log========================\n",
    "    if (epoch+1)%1==0:\n",
    "        print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "        \n",
    "        seven_total_inputs=torch.cat(\n",
    "            (band1.view(-1,1),band2.view(-1,1),band3.view(-1,1),band4.view(-1,1),band5.view(-1,1),band6.view(-1,1),band7.view(-1,1)),1)\n",
    "        print(seven_total_inputs.size())\n",
    "        seven_total_outputs=model(seven_total_inputs)\n",
    "        print(seven_total_outputs.size())\n",
    "        \n",
    "        gtiff_driver = gdal.GetDriverByName('GTiff')\n",
    "        out_ds = gtiff_driver.Create(\n",
    "            'out_seven_bands.tif',in_band.XSize, in_band.YSize, 7, in_band.DataType)\n",
    "        out_ds.SetProjection(in_ds.GetProjection())\n",
    "        out_ds.SetGeoTransform(in_ds.GetGeoTransform())\n",
    "        for j in range(7):\n",
    "            out_band = out_ds.GetRasterBand(j)\n",
    "            out_band.WriteArray(\n",
    "                seven_total_outputs[:][j].view(band1.size(0),band1.size(1)))\n",
    "        out_ds.FlushCache()\n",
    "        for k in range(1, 7):\n",
    "            out_ds.GetRasterBand(k).ComputeStatistics(False)\n",
    "        del out_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(band1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7591 7751 7 2\n",
      "PROJCS[\"WGS 84 / UTM zone 44N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",81],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32644\"]]\n",
      "(386385.0, 30.0, 0.0, 1873815.0, 0.0, -30.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "\n",
    "os.chdir('E:/pyScripts/pytorch/data/142_49/LC08_L1TP_142049_20170120_20170311_01_T1/')\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B1.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band1=in_band.ReadAsArray()\n",
    "\n",
    "print(in_band.XSize, in_band.YSize, 7, in_band.DataType)\n",
    "print(in_ds.GetProjection())\n",
    "print(in_ds.GetGeoTransform())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c06f87ac64fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[0min_band\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0min_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[0mband6\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_band\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReadAsArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m \u001b[0mband6\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_max_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mband6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-c06f87ac64fd>\u001b[0m in \u001b[0;36mmin_max_normalization\u001b[1;34m(tensor, min_value, max_value)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mmax_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmax_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_value\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:204"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(7, 3),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 7),\n",
    "            nn.ReLU(True)\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.chdir('E:/pyScripts/pytorch/data/142_49/LC08_L1TP_142049_20170120_20170311_01_T1/')\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B1.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band1=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band1=min_max_normalization(band1,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B2.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band2=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band2=min_max_normalization(band2,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B3.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band3=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band3=min_max_normalization(band3,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B4.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band4=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band4=min_max_normalization(band4,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B5.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band5=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band5=min_max_normalization(band5,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B6.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band6=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band6=min_max_normalization(band6,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B7.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band7=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band7=min_max_normalization(band7,0.0,1.0)\n",
    "\n",
    "\n",
    "num_epochs=10\n",
    "\n",
    "\n",
    "#batch_size=55\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for epoch in range(num_epochs):\n",
    "    atuple=[]\n",
    "    for i in range(0,band1.size(0)):\n",
    "        b1_input=band1[i].view(-1,1)\n",
    "        b2_input=band2[i].view(-1,1)\n",
    "        b3_input=band3[i].view(-1,1)\n",
    "        b4_input=band4[i].view(-1,1)\n",
    "        b5_input=band5[i].view(-1,1)\n",
    "        b6_input=band6[i].view(-1,1)\n",
    "        b7_input=band7[i].view(-1,1)\n",
    "        bands_input = torch.cat((\n",
    "            b1_input,b2_input,b3_input,b4_input,b5_input,b6_input,b7_input),1).float()\n",
    "        bands_input=bands_input.cuda()\n",
    "        output=model(bands_input)\n",
    "        loss = criterion(output, bands_input)\n",
    "        MSE_loss = nn.MSELoss()(output, bands_input)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "#         MSE_loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        atuple.append(output)\n",
    "        # ===================log========================\n",
    "    if (epoch+1)%1==0:\n",
    "        print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "    \n",
    "atuple=tuple(atuple)\n",
    "#print(len(atuple))\n",
    "inputdata = torch.cat(atuple,0)\n",
    "#print(inputdata.size())\n",
    "\n",
    "gtiff_driver = gdal.GetDriverByName('GTiff')\n",
    "out_ds = gtiff_driver.Create(\n",
    "    'out_seven_bands.tif',band1.size(1), band1.size(0), 7, in_band.DataType)\n",
    "out_ds.SetProjection(in_ds.GetProjection())\n",
    "# out_ds.SetGeoTransform((386385.0+30*3000, 30.0, 0.0, 1873815.0-30.0*3751, 0.0, -30.0),)\n",
    "out_ds.SetGeoTransform(in_ds.GetGeoTransform())\n",
    "for j in range(7):\n",
    "    out_band = out_ds.GetRasterBand((7-j))\n",
    "    readyData=(inputdata[:,j].view(band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "    out_band.WriteArray(readyData)\n",
    "out_ds.FlushCache()\n",
    "for k in range(1, 7):\n",
    "    out_ds.GetRasterBand(k).ComputeStatistics(False)\n",
    "del out_ds\n",
    "readyData_blue=(inputdata[:,2].view(1,band1.size(0),band1.size(1))).cpu()\n",
    "readyData_green=(inputdata[:,3].view(1,band1.size(0),band1.size(1))).cpu()\n",
    "readyData_red=(inputdata[:,4].view(1,band1.size(0),band1.size(1))).cpu()\n",
    "readyData_BGR=torch.cat((readyData_blue,readyData_green,readyData_red),0)\n",
    "save_image(readyData_BGR,'out_BRG.png')\n",
    "print('all set!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[700 x 500]' is invalid for input with 102000 elements at ..\\aten\\src\\TH\\THStorage.cpp:84",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-28fb4f8950e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mout_band\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mreadyData\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mband1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mband1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mout_band\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWriteArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreadyData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mout_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlushCache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 2: size '[700 x 500]' is invalid for input with 102000 elements at ..\\aten\\src\\TH\\THStorage.cpp:84"
     ]
    }
   ],
   "source": [
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for epoch in range(num_epochs):\n",
    "    atuple=[]\n",
    "    for i in range(0,band1.size(0)):\n",
    "        b1_input=band1[i].view(-1,1)\n",
    "        b2_input=band2[i].view(-1,1)\n",
    "        b3_input=band3[i].view(-1,1)\n",
    "        b4_input=band4[i].view(-1,1)\n",
    "        b5_input=band5[i].view(-1,1)\n",
    "        b6_input=band6[i].view(-1,1)\n",
    "        b7_input=band7[i].view(-1,1)\n",
    "        bands_input = torch.cat((\n",
    "            b1_input,b2_input,b3_input,b4_input,b5_input,b6_input,b7_input),1).float()\n",
    "        bands_input=bands_input.cuda()\n",
    "        output=model(bands_input)\n",
    "        loss = criterion(output, bands_input)\n",
    "        MSE_loss = nn.MSELoss()(output, bands_input)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "#         MSE_loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        atuple.append(output)\n",
    "        # ===================log========================\n",
    "    if (epoch+1)%1==0:\n",
    "        print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "    \n",
    "atuple=tuple(atuple)\n",
    "#print(len(atuple))\n",
    "inputdata = torch.cat(atuple,0)\n",
    "#print(inputdata.size())\n",
    "\n",
    "gtiff_driver = gdal.GetDriverByName('GTiff')\n",
    "out_ds = gtiff_driver.Create(\n",
    "    'out_seven_bands.tif',band1.size(1), band1.size(0), 7, in_band.DataType)\n",
    "out_ds.SetProjection(in_ds.GetProjection())\n",
    "out_ds.SetGeoTransform((386385.0+30*3000, 30.0, 0.0, 1873815.0-30.0*3751, 0.0, -30.0),)\n",
    "for j in range(7):\n",
    "    out_band = out_ds.GetRasterBand((7-j))\n",
    "    readyData=(inputdata[:,j].view(band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "    out_band.WriteArray(readyData)\n",
    "out_ds.FlushCache()\n",
    "for k in range(1, 7):\n",
    "    out_ds.GetRasterBand(k).ComputeStatistics(False)\n",
    "del out_ds\n",
    "readyData_blue=(inputdata[:,2].view(1,band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "readyData_green=(inputdata[:,3].view(1,band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "readyData_red=(inputdata[:,4].view(1,band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "readyData_BGR=torch.cat((readyData_blue,readyData_green,readyData_red),0)\n",
    "save_image(readyData_BGR,'out_BRG.png')\n",
    "print('all set!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Installations\\Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\ipykernel_launcher.py:132: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/300], loss:2.7804, MSE_loss:0.0474\n",
      "epoch [2/300], loss:2.7590, MSE_loss:0.0408\n",
      "epoch [3/300], loss:2.7455, MSE_loss:0.0368\n",
      "epoch [4/300], loss:2.7348, MSE_loss:0.0341\n",
      "epoch [5/300], loss:2.7262, MSE_loss:0.0323\n",
      "epoch [6/300], loss:2.7212, MSE_loss:0.0313\n",
      "epoch [7/300], loss:2.7193, MSE_loss:0.0309\n",
      "epoch [8/300], loss:2.7189, MSE_loss:0.0307\n",
      "epoch [9/300], loss:2.7187, MSE_loss:0.0306\n",
      "epoch [10/300], loss:2.7185, MSE_loss:0.0306\n",
      "epoch [11/300], loss:2.7183, MSE_loss:0.0305\n",
      "epoch [12/300], loss:2.7183, MSE_loss:0.0304\n",
      "epoch [13/300], loss:2.7185, MSE_loss:0.0304\n",
      "epoch [14/300], loss:2.7183, MSE_loss:0.0304\n",
      "epoch [15/300], loss:2.7185, MSE_loss:0.0304\n",
      "epoch [16/300], loss:2.7185, MSE_loss:0.0304\n",
      "epoch [17/300], loss:2.7183, MSE_loss:0.0304\n",
      "epoch [18/300], loss:2.7184, MSE_loss:0.0304\n",
      "epoch [19/300], loss:1.9267, MSE_loss:0.0295\n",
      "epoch [20/300], loss:1.9047, MSE_loss:0.0288\n",
      "epoch [21/300], loss:0.4840, MSE_loss:0.0187\n",
      "epoch [22/300], loss:0.4515, MSE_loss:0.0134\n",
      "epoch [23/300], loss:0.4310, MSE_loss:0.0089\n",
      "epoch [24/300], loss:0.4182, MSE_loss:0.0054\n",
      "epoch [25/300], loss:0.4121, MSE_loss:0.0035\n",
      "epoch [26/300], loss:0.4080, MSE_loss:0.0021\n",
      "epoch [27/300], loss:0.4054, MSE_loss:0.0012\n",
      "epoch [28/300], loss:0.4039, MSE_loss:0.0007\n",
      "epoch [29/300], loss:0.4031, MSE_loss:0.0004\n",
      "epoch [30/300], loss:0.4027, MSE_loss:0.0003\n",
      "epoch [31/300], loss:0.4025, MSE_loss:0.0002\n",
      "epoch [32/300], loss:0.4024, MSE_loss:0.0002\n",
      "epoch [33/300], loss:0.4025, MSE_loss:0.0002\n",
      "epoch [34/300], loss:0.4025, MSE_loss:0.0002\n",
      "epoch [35/300], loss:0.4026, MSE_loss:0.0003\n",
      "epoch [36/300], loss:0.4026, MSE_loss:0.0003\n",
      "epoch [37/300], loss:0.4027, MSE_loss:0.0003\n",
      "epoch [38/300], loss:0.4027, MSE_loss:0.0003\n",
      "epoch [39/300], loss:0.4027, MSE_loss:0.0003\n",
      "epoch [40/300], loss:0.4027, MSE_loss:0.0004\n",
      "epoch [41/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [42/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [43/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [44/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [45/300], loss:0.4027, MSE_loss:0.0004\n",
      "epoch [46/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [47/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [48/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [49/300], loss:0.4029, MSE_loss:0.0004\n",
      "epoch [50/300], loss:0.4029, MSE_loss:0.0004\n",
      "epoch [51/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [52/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [53/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [54/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [55/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [56/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [57/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [58/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [59/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [60/300], loss:0.4029, MSE_loss:0.0005\n",
      "epoch [61/300], loss:0.4028, MSE_loss:0.0005\n",
      "epoch [62/300], loss:0.4028, MSE_loss:0.0005\n",
      "epoch [63/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [64/300], loss:0.4028, MSE_loss:0.0004\n",
      "epoch [65/300], loss:0.4027, MSE_loss:0.0004\n",
      "epoch [66/300], loss:0.4026, MSE_loss:0.0004\n",
      "epoch [67/300], loss:0.4037, MSE_loss:0.0004\n",
      "epoch [68/300], loss:0.4028, MSE_loss:0.0003\n",
      "epoch [69/300], loss:0.4024, MSE_loss:0.0003\n",
      "epoch [70/300], loss:0.4023, MSE_loss:0.0002\n",
      "epoch [71/300], loss:0.4023, MSE_loss:0.0002\n",
      "epoch [72/300], loss:0.4022, MSE_loss:0.0002\n",
      "epoch [73/300], loss:0.4022, MSE_loss:0.0002\n",
      "epoch [74/300], loss:0.4022, MSE_loss:0.0002\n",
      "epoch [75/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [76/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [77/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [78/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [79/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [80/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [81/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [82/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [83/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [84/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [85/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [86/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [87/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [88/300], loss:0.4022, MSE_loss:0.0002\n",
      "epoch [89/300], loss:0.4022, MSE_loss:0.0002\n",
      "epoch [90/300], loss:0.4022, MSE_loss:0.0002\n",
      "epoch [91/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [92/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [93/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [94/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [95/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [96/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [97/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [98/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [99/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [100/300], loss:0.4022, MSE_loss:0.0002\n",
      "epoch [101/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [102/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [103/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [104/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [105/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [106/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [107/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [108/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [109/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [110/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [111/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [112/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [113/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [114/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [115/300], loss:0.4023, MSE_loss:0.0002\n",
      "epoch [116/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [117/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [118/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [119/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [120/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [121/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [122/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [123/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [124/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [125/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [126/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [127/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [128/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [129/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [130/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [131/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [132/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [133/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [134/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [135/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [136/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [137/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [138/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [139/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [140/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [141/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [142/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [143/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [144/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [145/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [146/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [147/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [148/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [149/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [150/300], loss:0.4020, MSE_loss:0.0001\n",
      "epoch [151/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [152/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [153/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [154/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [155/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [156/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [157/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [158/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [159/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [160/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [161/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [162/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [163/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [164/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [165/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [166/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [167/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [168/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [169/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [170/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [171/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [172/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [173/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [174/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [175/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [176/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [177/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [178/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [179/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [180/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [181/300], loss:0.4019, MSE_loss:0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [182/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [183/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [184/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [185/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [186/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [187/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [188/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [189/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [190/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [191/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [192/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [193/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [194/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [195/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [196/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [197/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [198/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [199/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [200/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [201/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [202/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [203/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [204/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [205/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [206/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [207/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [208/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [209/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [210/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [211/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [212/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [213/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [214/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [215/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [216/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [217/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [218/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [219/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [220/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [221/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [222/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [223/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [224/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [225/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [226/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [227/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [228/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [229/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [230/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [231/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [232/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [233/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [234/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [235/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [236/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [237/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [238/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [239/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [240/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [241/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [242/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [243/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [244/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [245/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [246/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [247/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [248/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [249/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [250/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [251/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [252/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [253/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [254/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [255/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [256/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [257/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [258/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [259/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [260/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [261/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [262/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [263/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [264/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [265/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [266/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [267/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [268/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [269/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [270/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [271/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [272/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [273/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [274/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [275/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [276/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [277/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [278/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [279/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [280/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [281/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [282/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [283/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [284/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [285/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [286/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [287/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [288/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [289/300], loss:0.4021, MSE_loss:0.0002\n",
      "epoch [290/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [291/300], loss:0.4020, MSE_loss:0.0002\n",
      "epoch [292/300], loss:0.4019, MSE_loss:0.0002\n",
      "epoch [293/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [294/300], loss:0.4019, MSE_loss:0.0001\n",
      "epoch [295/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [296/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [297/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [298/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [299/300], loss:0.4018, MSE_loss:0.0001\n",
      "epoch [300/300], loss:0.4018, MSE_loss:0.0001\n",
      "all set!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(7, 3),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 7),\n",
    "            nn.ReLU(True)\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.chdir('E:/pyScripts/pytorch/data/142_49/LC08_L1TP_142049_20170120_20170311_01_T1/')\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B1.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band1=torch.from_numpy(in_band.ReadAsArray().astype('float'))[4000:4700,3000:3500]\n",
    "band1=min_max_normalization(band1,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B2.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band2=torch.from_numpy(in_band.ReadAsArray().astype('float'))[4000:4700,3000:3500]\n",
    "band2=min_max_normalization(band2,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B3.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band3=torch.from_numpy(in_band.ReadAsArray().astype('float'))[4000:4700,3000:3500]\n",
    "band3=min_max_normalization(band3,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B4.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band4=torch.from_numpy(in_band.ReadAsArray().astype('float'))[4000:4700,3000:3500]\n",
    "band4=min_max_normalization(band4,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B5.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band5=torch.from_numpy(in_band.ReadAsArray().astype('float'))[4000:4700,3000:3500]\n",
    "band5=min_max_normalization(band5,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B6.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band6=torch.from_numpy(in_band.ReadAsArray().astype('float'))[4000:4700,3000:3500]\n",
    "band6=min_max_normalization(band6,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B7.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band7=torch.from_numpy(in_band.ReadAsArray().astype('float'))[4000:4700,3000:3500]\n",
    "band7=min_max_normalization(band7,0.0,1.0)\n",
    "\n",
    "\n",
    "num_epochs=300\n",
    "\n",
    "learning_rate = 1e-4\n",
    "#batch_size=55\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    atuple=[]\n",
    "    for i in range(0,band1.size(0)):\n",
    "        b1_input=band1[i].view(-1,1)\n",
    "        b2_input=band2[i].view(-1,1)\n",
    "        b3_input=band3[i].view(-1,1)\n",
    "        b4_input=band4[i].view(-1,1)\n",
    "        b5_input=band5[i].view(-1,1)\n",
    "        b6_input=band6[i].view(-1,1)\n",
    "        b7_input=band7[i].view(-1,1)\n",
    "        bands_input = torch.cat((\n",
    "            b1_input,b2_input,b3_input,b4_input,b5_input,b6_input,b7_input),1).float()\n",
    "        bands_input=bands_input.cuda()\n",
    "        output=model(bands_input)\n",
    "        loss = criterion(output, bands_input)\n",
    "        MSE_loss = nn.MSELoss()(output, bands_input)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "#         MSE_loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        atuple.append(output)\n",
    "        # ===================log========================\n",
    "    if (epoch+1)%1==0:\n",
    "        print('epoch [{}/{}], loss:{:.4f}, MSE_loss:{:.7f}'.format(epoch + 1, num_epochs, loss.data[0], MSE_loss.data[0]))\n",
    "    \n",
    "atuple=tuple(atuple)\n",
    "#print(len(atuple))\n",
    "inputdata = torch.cat(atuple,0)\n",
    "#print(inputdata.size())\n",
    "\n",
    "gtiff_driver = gdal.GetDriverByName('GTiff')\n",
    "out_ds = gtiff_driver.Create(\n",
    "    'out_seven_bands_times10000.tif',band1.size(1), band1.size(0), 7, in_band.DataType)\n",
    "out_ds.SetProjection(in_ds.GetProjection())\n",
    "out_ds.SetGeoTransform((386385.0+30*3000, 30.0, 0.0, 1873815.0-30.0*4000, 0.0, -30.0),)\n",
    "#out_ds.SetGeoTransform(in_ds.GetGeoTransform())\n",
    "for j in range(7):\n",
    "    out_band = out_ds.GetRasterBand((j+1))\n",
    "    readyData=(inputdata[:,j].view(band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "    readyData=readyData*10000\n",
    "    out_band.WriteArray(readyData)\n",
    "out_ds.FlushCache()\n",
    "for k in range(1, 7):\n",
    "    out_ds.GetRasterBand(k).ComputeStatistics(False)\n",
    "del out_ds\n",
    "readyData_blue=(inputdata[:,2].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_green=(inputdata[:,3].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_red=(inputdata[:,4].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_BGR=torch.cat((readyData_blue,readyData_green,readyData_red),0)\n",
    "save_image(readyData_BGR,'out_BRG.png')\n",
    "print('all set!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 500)\n",
      "(700, 500)\n",
      "(700, 500)\n",
      "all set!\n"
     ]
    }
   ],
   "source": [
    "hiddenlayer_tupple=[]\n",
    "for i in range(0,band1.size(0)):\n",
    "    b1_input=band1[i].view(-1,1)\n",
    "    b2_input=band2[i].view(-1,1)\n",
    "    b3_input=band3[i].view(-1,1)\n",
    "    b4_input=band4[i].view(-1,1)\n",
    "    b5_input=band5[i].view(-1,1)\n",
    "    b6_input=band6[i].view(-1,1)\n",
    "    b7_input=band7[i].view(-1,1)\n",
    "    bands_input = torch.cat((\n",
    "        b1_input,b2_input,b3_input,b4_input,b5_input,b6_input,b7_input),1).float()\n",
    "    bands_input=bands_input.cuda()\n",
    "    output=model.encoder(bands_input)\n",
    "    hiddenlayer_tupple.append(output)\n",
    "        \n",
    "hiddenlayer_tupple=tuple(hiddenlayer_tupple)\n",
    "#print(len(atuple))\n",
    "hiddenlayer_data = torch.cat(hiddenlayer_tupple,0)\n",
    "#print(inputdata.size())\n",
    "\n",
    "gtiff_driver = gdal.GetDriverByName('GTiff')\n",
    "out_ds = gtiff_driver.Create(\n",
    "    'out_hiddenlayers_times10000.tif',band1.size(1), band1.size(0), 3, in_band.DataType)\n",
    "out_ds.SetProjection(in_ds.GetProjection())\n",
    "out_ds.SetGeoTransform((386385.0+30*3000, 30.0, 0.0, 1873815.0-30.0*4000, 0.0, -30.0),)\n",
    "#out_ds.SetGeoTransform(in_ds.GetGeoTransform())\n",
    "for j in range(3):\n",
    "    out_band = out_ds.GetRasterBand((j+1))\n",
    "    readyData=(hiddenlayer_data[:,j].view(band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "    readyData=readyData*10000\n",
    "    print(readyData.shape)\n",
    "    out_band.WriteArray(readyData)\n",
    "out_ds.FlushCache()\n",
    "for k in range(1, 3):\n",
    "    out_ds.GetRasterBand(k).ComputeStatistics(False)\n",
    "del out_ds\n",
    "readyData_blue=(hiddenlayer_data[:,0].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_green=(hiddenlayer_data[:,1].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_red=(hiddenlayer_data[:,2].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_BGR=torch.cat((readyData_blue,readyData_green,readyData_red),0)\n",
    "save_image(readyData_BGR,'out_hidden_BRG.png')\n",
    "print('all set!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c7ed7f6c3f46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0min_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgdal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mband_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0min_band\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0min_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetRasterBand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mband1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_band\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReadAsArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mband1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_max_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mband1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B1.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band1=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band1=min_max_normalization(band1,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B2.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band2=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band2=min_max_normalization(band2,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B3.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band3=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band3=min_max_normalization(band3,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B4.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band4=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band4=min_max_normalization(band4,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B5.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band5=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band5=min_max_normalization(band5,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B6.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band6=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band6=min_max_normalization(band6,0.0,1.0)\n",
    "\n",
    "\n",
    "band_fn='LC08_L1TP_142049_20170120_20170311_01_T1_B7.TIF'\n",
    "in_ds = gdal.Open(band_fn)\n",
    "in_band=in_ds.GetRasterBand(1)\n",
    "band7=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "band7=min_max_normalization(band7,0.0,1.0)\n",
    "\n",
    "\n",
    "outlayers_tupple = []\n",
    "hiddenlayer_tupple=[]\n",
    "for i in range(0,band1.size(0)):\n",
    "    b1_input=band1[i].view(-1,1)\n",
    "    b2_input=band2[i].view(-1,1)\n",
    "    b3_input=band3[i].view(-1,1)\n",
    "    b4_input=band4[i].view(-1,1)\n",
    "    b5_input=band5[i].view(-1,1)\n",
    "    b6_input=band6[i].view(-1,1)\n",
    "    b7_input=band7[i].view(-1,1)\n",
    "    bands_input = torch.cat((\n",
    "        b1_input,b2_input,b3_input,b4_input,b5_input,b6_input,b7_input),1).float()\n",
    "    bands_input=bands_input.cuda()\n",
    "    \n",
    "    outputlayers=model(bands_input)\n",
    "#     output_hidden=model.encoder(bands_input)\n",
    "    \n",
    "    outlayers_tupple.append(outputlayers)\n",
    "#     hiddenlayer_tupple.append(output_hidden)\n",
    "\n",
    "outlayers_tupple=tuple(outlayers_tupple)\n",
    "# hiddenlayer_tupple=tuple(hiddenlayer_tupple)\n",
    "\n",
    "outlayer_data = torch.cat(outlayers_tupple,0)\n",
    "# hiddenlayer_data = torch.cat(hiddenlayer_tupple,0)\n",
    "\n",
    "\n",
    "gtiff_driver = gdal.GetDriverByName('GTiff')\n",
    "out_ds = gtiff_driver.Create(\n",
    "    'out_wholeIMG_times10000.tif',band1.size(1), band1.size(0), 7, in_band.DataType)\n",
    "out_ds.SetProjection(in_ds.GetProjection())\n",
    "out_ds.SetGeoTransform(in_ds.GetGeoTransform())\n",
    "for j in range(7):\n",
    "    out_band = out_ds.GetRasterBand((j+1))\n",
    "    readyData=(outlayer_data[:,j].view(band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "    readyData=readyData*10000\n",
    "    print(readyData.shape)\n",
    "    out_band.WriteArray(readyData)\n",
    "out_ds.FlushCache()\n",
    "for k in range(1, 7):\n",
    "    out_ds.GetRasterBand(k).ComputeStatistics(False)\n",
    "del out_ds\n",
    "readyData_blue=(outlayer_data[:,1].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_green=(outlayer_data[:,2].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_red=(outlayer_data[:,3].view(1,band1.size(0),band1.size(1)))\n",
    "readyData_BGR=torch.cat((readyData_blue,readyData_green,readyData_red),0)\n",
    "save_image(readyData_BGR,'out_wholeIMG_BRG.png')\n",
    "\n",
    "\n",
    "\n",
    "# gtiff_driver = gdal.GetDriverByName('GTiff')\n",
    "# out_ds = gtiff_driver.Create(\n",
    "#     'out_hiddenwholeIMG_times10000.tif',band1.size(1), band1.size(0), 3, in_band.DataType)\n",
    "# out_ds.SetProjection(in_ds.GetProjection())\n",
    "# out_ds.SetGeoTransform((386385.0+30*3000, 30.0, 0.0, 1873815.0-30.0*4000, 0.0, -30.0),)\n",
    "# #out_ds.SetGeoTransform(in_ds.GetGeoTransform())\n",
    "# for j in range(3):\n",
    "#     out_band = out_ds.GetRasterBand((j+1))\n",
    "#     readyData=(hiddenlayer_data[:,j].view(band1.size(0),band1.size(1))).cpu().detach().numpy()\n",
    "#     readyData=readyData*10000\n",
    "#     print(readyData.shape)\n",
    "#     out_band.WriteArray(readyData)\n",
    "# out_ds.FlushCache()\n",
    "# for k in range(1, 3):\n",
    "#     out_ds.GetRasterBand(k).ComputeStatistics(False)\n",
    "# del out_ds\n",
    "# readyData_blue=(hiddenlayer_data[:,0].view(1,band1.size(0),band1.size(1)))\n",
    "# readyData_green=(hiddenlayer_data[:,1].view(1,band1.size(0),band1.size(1)))\n",
    "# readyData_red=(hiddenlayer_data[:,2].view(1,band1.size(0),band1.size(1)))\n",
    "# readyData_BGR=torch.cat((readyData_blue,readyData_green,readyData_red),0)\n",
    "# save_image(readyData_BGR,'out_hiddenwholeIMG_BRG.png')\n",
    "\n",
    "\n",
    "\n",
    "print('all set!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy\n",
    "from PIL import Image\n",
    "import torch\n",
    "from osgeo import gdal\n",
    "\n",
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    return tensor\n",
    "\n",
    "os.chdir('E:/pyScripts/pytorch/data/142_49/LC08_L1TP_142049_20170120_20170311_01_T1/')\n",
    "raw=[]\n",
    "for i in range(7):\n",
    "    band_fn='windowed_LC08_L1TP_142049_20170120_20170311_01_T1_B'+str(i+1)+'.rst'\n",
    "    in_ds = gdal.Open(band_fn)\n",
    "    in_band=in_ds.GetRasterBand(1)\n",
    "    band=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "    band=min_max_normalization(band,0.0,1.0)\n",
    "    raw.append(band)\n",
    "\n",
    "pca_inverse=[]\n",
    "for i in range(7):\n",
    "    band_fn='inverse_pca'+str(i+1)+'.rst'\n",
    "    in_ds = gdal.Open(band_fn)\n",
    "    in_band=in_ds.GetRasterBand(1)\n",
    "    band=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "    band=min_max_normalization(band,0.0,1.0)\n",
    "    pca_inverse.append(band)\n",
    "    \n",
    "ae_reconstructed=[]\n",
    "for i in range(7):\n",
    "    band_fn='AE_outputs_whole_b'+str(i+1)+'.rst'\n",
    "    in_ds = gdal.Open(band_fn)\n",
    "    in_band=in_ds.GetRasterBand(1)\n",
    "    band=torch.from_numpy(in_band.ReadAsArray().astype('float'))\n",
    "    band=min_max_normalization(band,0.0,1.0)\n",
    "    ae_reconstructed.append(band)\n",
    "\n",
    "MSE_loss_pca = []\n",
    "BCE_loss_pca = []\n",
    "for i in range(7):\n",
    "    loss=nn.MSELoss()(pca_inverse[0], raw[0])\n",
    "    bceloss=nn.BCELoss()(pca_inverse[0], raw[0])\n",
    "    MSE_loss_pca.append(loss)\n",
    "    BCE_loss_pca.append(bceloss)\n",
    "    \n",
    "MSE_loss_ae = []\n",
    "BCE_loss_ae = []\n",
    "for i in range(7):\n",
    "    loss=nn.MSELoss(size_average=False)(ae_reconstructed[0], raw[0])\n",
    "    bceloss=nn.BCELoss()(ae_reconstructed[0], raw[0])\n",
    "    MSE_loss_ae.append(loss)\n",
    "    BCE_loss_ae.append(bceloss)\n",
    "\n",
    "# for i in range(7):\n",
    "#     print('MSE Loss of AE: ', MSE_loss_ae[i], 'MSE Loss of PCA: ', MSE_loss_pca[i])\n",
    "#     print('Cross Entropy Loss of AE: ', BCE_loss_ae[i], 'Cross Entropy Loss of PCA: ', BCE_loss_pca[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25583.9812, dtype=torch.float64)\n",
      "tensor(9314.7646, dtype=torch.float64)\n",
      "tensor(2714.9704, dtype=torch.float64)\n",
      "tensor(27777.6520, dtype=torch.float64)\n",
      "tensor(1012.9957, dtype=torch.float64)\n",
      "tensor(880.2447, dtype=torch.float64)\n",
      "tensor(1292.6850, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Installations\\Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# for i in range(7):\n",
    "#     print(nn.BCELoss(size_average=False)(pca_inverse[i], raw[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 4000]) torch.Size([4000, 4000]) torch.Size([4000, 4000])\n"
     ]
    }
   ],
   "source": [
    "print(raw[0].size(), pca_inverse[0].size(), ae_reconstructed[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.942\n",
      "Model:                            OLS   Adj. R-squared:                  0.942\n",
      "Method:                 Least Squares   F-statistic:                 2.613e+08\n",
      "Date:                Wed, 31 Oct 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:03:26   Log-Likelihood:             4.1506e+07\n",
      "No. Observations:            16000000   AIC:                        -8.301e+07\n",
      "Df Residuals:                15999998   BIC:                        -8.301e+07\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.0454    8.9e-06  -5094.738      0.000      -0.045      -0.045\n",
      "x              1.0896   6.74e-05   1.62e+04      0.000       1.089       1.090\n",
      "==============================================================================\n",
      "Omnibus:                   691788.738   Durbin-Watson:                   0.025\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           577672.904\n",
      "Skew:                          -0.392   Prob(JB):                         0.00\n",
      "Kurtosis:                       2.498   Cond. No.                         15.1\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.982\n",
      "Model:                            OLS   Adj. R-squared:                  0.982\n",
      "Method:                 Least Squares   F-statistic:                 8.651e+08\n",
      "Date:                Wed, 31 Oct 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:03:32   Log-Likelihood:             5.1529e+07\n",
      "No. Observations:            16000000   AIC:                        -1.031e+08\n",
      "Df Residuals:                15999998   BIC:                        -1.031e+08\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.0241   4.15e-06  -5804.966      0.000      -0.024      -0.024\n",
      "x              1.0210   3.47e-05   2.94e+04      0.000       1.021       1.021\n",
      "==============================================================================\n",
      "Omnibus:                  2099058.137   Durbin-Watson:                   0.081\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          3377895.810\n",
      "Skew:                          -0.916   Prob(JB):                         0.00\n",
      "Kurtosis:                       4.307   Cond. No.                         14.5\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.969\n",
      "Model:                            OLS   Adj. R-squared:                  0.969\n",
      "Method:                 Least Squares   F-statistic:                 5.082e+08\n",
      "Date:                Wed, 31 Oct 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:03:37   Log-Likelihood:             4.8228e+07\n",
      "No. Observations:            16000000   AIC:                        -9.646e+07\n",
      "Df Residuals:                15999998   BIC:                        -9.646e+07\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0067   4.75e-06   1409.384      0.000       0.007       0.007\n",
      "x              0.9826   4.36e-05   2.25e+04      0.000       0.983       0.983\n",
      "==============================================================================\n",
      "Omnibus:                  1210685.503   Durbin-Watson:                   0.064\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           851703.795\n",
      "Skew:                           0.458   Prob(JB):                         0.00\n",
      "Kurtosis:                       2.339   Cond. No.                         14.8\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.948\n",
      "Model:                            OLS   Adj. R-squared:                  0.948\n",
      "Method:                 Least Squares   F-statistic:                 2.891e+08\n",
      "Date:                Wed, 31 Oct 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:03:43   Log-Likelihood:             4.2610e+07\n",
      "No. Observations:            16000000   AIC:                        -8.522e+07\n",
      "Df Residuals:                15999998   BIC:                        -8.522e+07\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0394   4.82e-06   8174.534      0.000       0.039       0.039\n",
      "x              0.9658   5.68e-05    1.7e+04      0.000       0.966       0.966\n",
      "==============================================================================\n",
      "Omnibus:                  1466918.074   Durbin-Watson:                   0.041\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1902639.835\n",
      "Skew:                           0.831   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.305   Cond. No.                         13.5\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.999\n",
      "Model:                            OLS   Adj. R-squared:                  0.999\n",
      "Method:                 Least Squares   F-statistic:                 1.850e+10\n",
      "Date:                Wed, 31 Oct 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:03:49   Log-Likelihood:             6.1773e+07\n",
      "No. Observations:            16000000   AIC:                        -1.235e+08\n",
      "Df Residuals:                15999998   BIC:                        -1.235e+08\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0075   1.67e-06   4453.249      0.000       0.007       0.007\n",
      "x              0.9697   7.13e-06   1.36e+05      0.000       0.970       0.970\n",
      "==============================================================================\n",
      "Omnibus:                  5742702.817   Durbin-Watson:                   0.070\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         34426792.304\n",
      "Skew:                           1.610   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.424   Cond. No.                         5.73\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.995\n",
      "Model:                            OLS   Adj. R-squared:                  0.995\n",
      "Method:                 Least Squares   F-statistic:                 3.239e+09\n",
      "Date:                Wed, 31 Oct 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:03:54   Log-Likelihood:             6.8050e+07\n",
      "No. Observations:            16000000   AIC:                        -1.361e+08\n",
      "Df Residuals:                15999998   BIC:                        -1.361e+08\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0055   1.19e-06   4585.580      0.000       0.005       0.005\n",
      "x              1.0221    1.8e-05   5.69e+04      0.000       1.022       1.022\n",
      "==============================================================================\n",
      "Omnibus:                  8750941.692   Durbin-Watson:                   0.115\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       1745809139.936\n",
      "Skew:                          -1.538   Prob(JB):                         0.00\n",
      "Kurtosis:                      54.081   Cond. No.                         20.9\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.981\n",
      "Model:                            OLS   Adj. R-squared:                  0.981\n",
      "Method:                 Least Squares   F-statistic:                 8.168e+08\n",
      "Date:                Wed, 31 Oct 2018   Prob (F-statistic):               0.00\n",
      "Time:                        17:04:00   Log-Likelihood:             6.1598e+07\n",
      "No. Observations:            16000000   AIC:                        -1.232e+08\n",
      "Df Residuals:                15999998   BIC:                        -1.232e+08\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.0100      2e-06  -5026.482      0.000      -0.010      -0.010\n",
      "x              1.0787   3.77e-05   2.86e+04      0.000       1.079       1.079\n",
      "==============================================================================\n",
      "Omnibus:                  4084224.079   Durbin-Watson:                   0.148\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         53106342.857\n",
      "Skew:                           0.866   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.756   Cond. No.                         29.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "import pandas\n",
    "\n",
    "for i in range(7):\n",
    "    x = ae_reconstructed[i].view(-1).numpy()\n",
    "    y= raw[i].view(-1).detach().numpy()\n",
    "    data = pandas.DataFrame({'x': x, 'y': y})\n",
    "    model = ols(\"y ~ x\", data).fit()\n",
    "    print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
